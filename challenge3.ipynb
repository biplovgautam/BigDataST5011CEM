{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "269c04af-89fe-406a-972d-7cbfee6e6558",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found existing installation: pyspark 4.0.1\n",
      "Can't uninstall 'pyspark'. No files were found to uninstall.\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: pyspark in /opt/spark-4.0.1/python (4.0.1)\n",
      "Requirement already satisfied: py4j==0.10.9.9 in /Users/biplovgautam/Library/Python/3.9/lib/python/site-packages (from pyspark) (0.10.9.9)\n",
      "\u001b[33mWARNING: You are using pip version 21.2.4; however, version 25.3 is available.\n",
      "You should consider upgrading via the '/Applications/Xcode.app/Contents/Developer/usr/bin/python3 -m pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "\n",
    "!pip3 install pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3c634004-3494-4b23-8a78-587b4a497da6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['PYSPARK_PYTHON'] = 'python'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ef0860c9-9f43-4bac-8f4c-ef4d83f0068c",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'py4j'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpyspark\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msql\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m SparkSession\n\u001b[1;32m      2\u001b[0m spark \u001b[38;5;241m=\u001b[39m SparkSession\u001b[38;5;241m.\u001b[39mbuilder\u001b[38;5;241m.\u001b[39mappName(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mjfktosea\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39mgetOrCreate()\n",
      "File \u001b[0;32m/opt/spark-4.0.1/python/pyspark/__init__.py:58\u001b[0m\n\u001b[1;32m     56\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpyspark\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mrdd\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m RDD, RDDBarrier\n\u001b[1;32m     57\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpyspark\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfiles\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m SparkFiles\n\u001b[0;32m---> 58\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpyspark\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mstatus\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m StatusTracker, SparkJobInfo, SparkStageInfo\n\u001b[1;32m     59\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpyspark\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbroadcast\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Broadcast\n\u001b[1;32m     60\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpyspark\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m rdd, files, status, broadcast\n",
      "File \u001b[0;32m/opt/spark-4.0.1/python/pyspark/core/status.py:22\u001b[0m\n\u001b[1;32m     18\u001b[0m __all__ \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSparkJobInfo\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSparkStageInfo\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mStatusTracker\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m     20\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtyping\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m List, NamedTuple, Optional\n\u001b[0;32m---> 22\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpy4j\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mjava_collections\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m JavaArray\n\u001b[1;32m     23\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpy4j\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mjava_gateway\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m JavaObject\n\u001b[1;32m     26\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mSparkJobInfo\u001b[39;00m(NamedTuple):\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'py4j'"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.appName(\"jfktosea\").getOrCreate() # name should be quniue for less conflicts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f3f92226-c30a-479d-9f74-924f6748fee0",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'py4j'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpyspark\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msql\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m SparkSession\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpyspark\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msql\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfunctions\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m avg, \u001b[38;5;28mmax\u001b[39m, \u001b[38;5;28mmin\u001b[39m, col, \u001b[38;5;28mround\u001b[39m, isnan\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# --- 1. Initialize Spark Session ---\u001b[39;00m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;66;03m# Create a SparkSession instance.\u001b[39;00m\n",
      "File \u001b[0;32m/opt/spark-4.0.1/python/pyspark/__init__.py:58\u001b[0m\n\u001b[1;32m     56\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpyspark\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mrdd\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m RDD, RDDBarrier\n\u001b[1;32m     57\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpyspark\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfiles\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m SparkFiles\n\u001b[0;32m---> 58\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpyspark\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mstatus\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m StatusTracker, SparkJobInfo, SparkStageInfo\n\u001b[1;32m     59\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpyspark\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbroadcast\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Broadcast\n\u001b[1;32m     60\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpyspark\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m rdd, files, status, broadcast\n",
      "File \u001b[0;32m/opt/spark-4.0.1/python/pyspark/core/status.py:22\u001b[0m\n\u001b[1;32m     18\u001b[0m __all__ \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSparkJobInfo\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSparkStageInfo\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mStatusTracker\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m     20\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtyping\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m List, NamedTuple, Optional\n\u001b[0;32m---> 22\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpy4j\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mjava_collections\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m JavaArray\n\u001b[1;32m     23\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpy4j\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mjava_gateway\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m JavaObject\n\u001b[1;32m     26\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mSparkJobInfo\u001b[39;00m(NamedTuple):\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'py4j'"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import avg, max, min, col, round, isnan\n",
    "\n",
    "# --- 1. Initialize Spark Session ---\n",
    "# Create a SparkSession instance.\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"JFK_to_SEA_Airtime_Analysis_SQL\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Set logging level to WARN to reduce console output clutter\n",
    "spark.sparkContext.setLogLevel(\"WARN\")\n",
    "\n",
    "# --- 2. Define Schema and Load Data ---\n",
    "# IMPORTANT: Replace 'path/to/your/nyc_flights.csv' with the actual file path.\n",
    "FILE_PATH = 'path/to/your/nyc_flights.csv'\n",
    "\n",
    "try:\n",
    "    # Load the flights data\n",
    "    flights_df = spark.read.csv(\n",
    "        FILE_PATH,\n",
    "        header=True,\n",
    "        inferSchema=True\n",
    "    )\n",
    "    print(f\"Successfully loaded data from: {FILE_PATH}\")\n",
    "    \n",
    "    # --- 3. Register DataFrame as a Temporary SQL View ---\n",
    "    # We must register the DataFrame so that Spark SQL can query it.\n",
    "    flights_df.createOrReplaceTempView(\"flights\")\n",
    "    print(\"DataFrame registered as SQL view 'flights'.\")\n",
    "\n",
    "    # --- 4. Define and Execute Spark SQL Query (Challenge 3) ---\n",
    "    # Challenge 3: Find the average, maximum, and minimum airtime for the JFK to SEA route.\n",
    "    \n",
    "    # The query filters by Origin='JFK' and Dest='SEA'.\n",
    "    # It casts 'air_time' to INT and filters out NULLs to ensure accurate aggregation.\n",
    "    query3 = \"\"\"\n",
    "    SELECT\n",
    "        ROUND(AVG(CAST(air_time AS INT)), 2) AS Average_AirTime_Minutes,\n",
    "        MAX(CAST(air_time AS INT)) AS Maximum_AirTime_Minutes,\n",
    "        MIN(CAST(air_time AS INT)) AS Minimum_AirTime_Minutes\n",
    "    FROM\n",
    "        flights\n",
    "    WHERE\n",
    "        origin = 'JFK' AND dest = 'SEA' AND air_time IS NOT NULL\n",
    "    \"\"\"\n",
    "\n",
    "    print(\"\\n--- Executing Spark SQL Query for JFK -> SEA Airtime Statistics ---\")\n",
    "    result3 = spark.sql(query3)\n",
    "\n",
    "    # --- 5. Display Results ---\n",
    "    # The result will be a single row containing the three required metrics.\n",
    "    result3.show(truncate=False)\n",
    "\n",
    "    # Optional: Count total valid flights (using the registered view for count)\n",
    "    count_query = \"\"\"\n",
    "    SELECT COUNT(*) AS total_flights\n",
    "    FROM flights\n",
    "    WHERE origin = 'JFK' AND dest = 'SEA' AND air_time IS NOT NULL\n",
    "    \"\"\"\n",
    "    count_df = spark.sql(count_query)\n",
    "    total_flights = count_df.collect()[0]['total_flights']\n",
    "    print(f\"\\nTotal valid JFK -> SEA flights included in this analysis: {total_flights}\")\n",
    "\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"\\n--- ERROR: Could not process the file. ---\")\n",
    "    print(f\"Please check the FILE_PATH: '{FILE_PATH}' and ensure your Spark environment is set up correctly.\")\n",
    "    print(f\"Detailed Error: {e}\")\n",
    "\n",
    "# --- 6. Stop Spark Session ---\n",
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d076c22c-2d0e-46b0-b0d5-0f29184097ef",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
